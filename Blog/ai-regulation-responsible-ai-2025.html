<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1, viewport-fit=cover" name="viewport"/>
<title>AI Regulation and Responsible AI in 2025: Governing Innovation with Ethics — DedSec Blog</title>
<meta content="Examines the EU AI Act’s risk‑based approach, fairness and bias mitigation, AI governance standards like ISO 42001, and the global patchwork of AI regulations." name="description"/>
<link href="https://ded-sec.space/Blog/ai-regulation-responsible-ai-2025.html" rel="canonical"/>
<meta content="2026-01-26" property="article:published_time"/>
    <meta content="Law" property="article:section"/>
    <meta content="Artificial Intelligence" property="article:tag"/>
<meta content="Regulation" property="article:tag"/>
<meta content="Fairness" property="article:tag"/>
<meta content="Responsible AI" property="article:tag"/>
<meta content="Ethics" property="article:tag"/>
    
<link href="https://fonts.googleapis.com" rel="preconnect"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css2?family=Orbitron:wght@500;700&family=Roboto+Mono:wght@400;600;700&display=swap" rel="stylesheet"/>
<link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" rel="stylesheet"/>
<link href="../style.css" rel="stylesheet"/>
<meta content="article" property="og:type"/>
<meta content="https://ded-sec.space/Blog/ai-regulation-responsible-ai-2025.html" property="og:url"/>
<meta content="AI Regulation and Responsible AI in 2025: Governing Innovation with Ethics | DedSec Blog" property="og:title"/>
<meta content="Examines the EU AI Act’s risk‑based approach, fairness and bias mitigation, AI governance standards like ISO 42001, and the global patchwork of AI regulations." property="og:description"/>
<meta content="https://ded-sec.space/Assets/Images/og/og-dark.jpg" property="og:image"/>
<meta content="summary_large_image" name="twitter:card"/>
<meta content="AI Regulation and Responsible AI in 2025: Governing Innovation with Ethics | DedSec Blog" name="twitter:title"/>
<meta content="Examines the EU AI Act’s risk‑based approach, fairness and bias mitigation, AI governance standards like ISO 42001, and the global patchwork of AI regulations." name="twitter:description"/>
<meta content="https://ded-sec.space/Assets/Images/og/og-dark.jpg" name="twitter:image"/>
<meta content="DedSec Project" property="og:site_name"/>
<meta content="en_US" property="og:locale"/>
<meta content="el_GR" property="og:locale:alternate"/>
<meta content="DedSec Project — cybersecurity and Termux learning resources" property="og:image:alt"/>
<meta content="DedSec Project — cybersecurity and Termux learning resources" name="twitter:image:alt"/>
<meta content="https://ded-sec.space/Blog/ai-regulation-responsible-ai-2025.html" name="twitter:url"/>
</head>
<body class="dark-theme">
<nav class="main-nav">
<div class="nav-container">
<div class="nav-title">
<h1>DedSec Project</h1>
</div>
<button aria-label="Toggle navigation" class="burger-menu" id="burger-menu">
<span class="burger-line"></span>
<span class="burger-line"></span>
<span class="burger-line"></span>
<span class="burger-label" data-en="Menu" data-gr="Μενού">Menu</span>
</button>
<div class="nav-menu" id="nav-menu">
<a class="nav-link" data-en="Home" data-gr="Αρχική" href="/">Home</a>
<a class="nav-link" data-en="Learn About The Tools" data-gr="Μάθετε για τα Εργαλεία" href="../Pages/learn-about-the-tools.html">Learn About The Tools</a>
<a class="nav-link" data-en="Guide For Installation" data-gr="Οδηγός Εγκατάστασης" href="../Pages/guide-for-installation.html">Guide For Installation</a>
<a class="nav-link" data-en="Frequently Asked Questions" data-gr="Συχνές Ερωτήσεις" href="../Pages/faq.html">Frequently Asked Questions</a>
<a class="nav-link" data-en="Store" data-gr="Κατάστημα" href="../Pages/store.html">Store</a><a class="nav-link active" data-en="Blog" data-gr="Ιστολόγιο" href="../Pages/blog.html">Blog</a>
<a class="nav-link" data-en="Collaborations" data-gr="Συνεργασίες" href="../Pages/collaborations.html">Collaborations</a>
<a class="nav-link" data-en="Portfolio & GitHub" data-gr="Portfolio & GitHub" href="../Pages/portfolio-github-info.html">Portfolio & GitHub</a>
<a class="nav-link" data-en="Contact & Credits" data-gr="Επικοινωνία & Συντελεστές" href="../Pages/contact-credits.html">Contact & Credits</a>
<a class="nav-link" data-en="Privacy Policy" data-gr="Πολιτική Απορρήτου" href="../Pages/privacy-policy.html">Privacy Policy</a>
</div>
<div class="nav-actions">
<button class="nav-action-btn" id="nav-theme-switcher">
<span class="nav-action-label" data-en="Change Theme" data-gr="Αλλαγή Θέματος">Change Theme</span>
</button>
<button class="nav-action-btn" id="nav-lang-switcher">
<span class="nav-action-label" data-en="Αλλαγή Γλώσσας" data-gr="Change Language">Αλλαγή Γλώσσας</span>
</button>
</div>
</div>
</nav>
<article class="blog-article page-content">
<header class="page-header">
<h1 data-en="AI Regulation and Responsible AI in 2025: Governing Innovation with Ethics" data-gr="Ρύθμιση AI και Υπεύθυνη AI το 2025: Διακυβέρνηση της Καινοτομίας με Ηθική">AI Regulation and Responsible AI in 2025: Governing Innovation with Ethics</h1>
<p data-en="Examines the EU AI Act’s risk‑based approach, fairness and bias mitigation, AI governance standards like ISO 42001, and the global patchwork of AI regulations." data-gr="Εξετάζει την προσέγγιση του Νόμου AI της ΕΕ με βάση τον κίνδυνο, τη μείωση προκατάληψης, τα πρότυπα διακυβέρνησης AI όπως το ISO 42001 και το παγκόσμιο μωσαϊκό κανονισμών για την AI.">Examines the EU AI Act’s risk‑based approach, fairness and bias mitigation, AI governance standards like ISO 42001, and the global patchwork of AI regulations.</p>
<div class="article-meta">
<span class="badge">Law</span>
<span data-en="10 min read" data-gr="10 λεπτά ανάγνωση">10 min read</span>
<span>2026-01-26</span>
</div>
</header>
<div class="article-content">
<section class="content-section">
<h2 data-en="The EU AI Act and Risk‑Based Regulation" data-gr="Ο Νόμος AI της ΕΕ και η Ρύθμιση Βασισμένη στον Κίνδυνο">The EU AI Act and Risk‑Based Regulation</h2>
<p data-en="In June 2024 the European Union adopted the AI Act, the world’s first comprehensive regulation of artificial intelligence. The law introduces a risk‑based framework that classifies AI systems as minimal, limited, high or unacceptable. Minimal‑risk systems, such as spam filters, face few obligations, while limited‑risk systems must comply with transparency requirements. High‑risk systems — including AI used for recruitment, credit scoring, education, law enforcement and critical infrastructure — are subject to strict obligations such as risk management, data governance, human oversight and post‑market monitoring. Unacceptable uses, such as manipulative techniques that exploit vulnerabilities, social scoring by governments and real‑time remote biometric identification, are banned outright【945144035004864†L149-L157】【960195212561326†L1000-L1023】. The AI Act’s first enforcement phase begins in mid‑2025. It reflects the EU’s commitment to safeguarding fundamental rights while promoting innovation. The regulation also introduces a conformity assessment scheme and market surveillance to ensure compliance. Other jurisdictions are watching closely, and many analysts expect the Act’s risk‑based framework to influence future AI laws worldwide." data-gr="Τον Ιούνιο 2024 η Ευρωπαϊκή Ένωση υιοθέτησε τον Νόμο AI, τον πρώτο ολοκληρωμένο κανονισμό για την τεχνητή νοημοσύνη στον κόσμο. Ο νόμος εισάγει ένα πλαίσιο βασισμένο στον κίνδυνο που ταξινομεί τα συστήματα AI ως ελάχιστου, περιορισμένου, υψηλού ή απαράδεκτου κινδύνου. Τα συστήματα ελάχιστου κινδύνου, όπως τα φίλτρα ανεπιθύμητης αλληλογραφίας, αντιμετωπίζουν λίγες υποχρεώσεις, ενώ τα συστήματα περιορισμένου κινδύνου πρέπει να συμμορφώνονται με απαιτήσεις διαφάνειας. Τα συστήματα υψηλού κινδύνου — συμπεριλαμβανομένης της AI που χρησιμοποιείται για πρόσληψη, αξιολόγηση πιστοληπτικής ικανότητας, εκπαίδευση, επιβολή νόμου και κρίσιμες υποδομές — υπόκεινται σε αυστηρές υποχρεώσεις όπως διαχείριση κινδύνου, διακυβέρνηση δεδομένων, ανθρώπινη εποπτεία και μεταγενέστερη παρακολούθηση. Οι απαράδεκτες χρήσεις, όπως οι χειριστικές τεχνικές που εκμεταλλεύονται ευάλωτες ομάδες, το κοινωνικό σκόρ από κυβερνήσεις και η απομακρυσμένη βιομετρική ταυτοποίηση σε πραγματικό χρόνο, απαγορεύονται【945144035004864†L149-L157】【960195212561326†L1000-L1023】. Η πρώτη φάση εφαρμογής του Νόμου AI ξεκινά στα μέσα του 2025. Αντικατοπτρίζει τη δέσμευση της ΕΕ να προστατεύσει τα θεμελιώδη δικαιώματα ενώ προωθεί την καινοτομία. Ο κανονισμός εισάγει επίσης ένα σύστημα αξιολόγησης συμμόρφωσης και εποπτείας της αγοράς για να διασφαλίσει την εφαρμογή. Άλλες δικαιοδοσίες παρακολουθούν στενά και πολλοί αναλυτές αναμένουν ότι το πλαίσιο του νόμου θα επηρεάσει τους μελλοντικούς νόμους AI παγκοσμίως.">In June 2024 the European Union adopted the AI Act, the world’s first comprehensive regulation of artificial intelligence. The law introduces a risk‑based framework that classifies AI systems as minimal, limited, high or unacceptable. Minimal‑risk systems, such as spam filters, face few obligations, while limited‑risk systems must comply with transparency requirements. High‑risk systems — including AI used for recruitment, credit scoring, education, law enforcement and critical infrastructure — are subject to strict obligations such as risk management, data governance, human oversight and post‑market monitoring. Unacceptable uses, such as manipulative techniques that exploit vulnerabilities, social scoring by governments and real‑time remote biometric identification, are banned outright【945144035004864†L149-L157】【960195212561326†L1000-L1023】. The AI Act’s first enforcement phase begins in mid‑2025. It reflects the EU’s commitment to safeguarding fundamental rights while promoting innovation. The regulation also introduces a conformity assessment scheme and market surveillance to ensure compliance. Other jurisdictions are watching closely, and many analysts expect the Act’s risk‑based framework to influence future AI laws worldwide.</p>
</section>
<section class="content-section">
<h2 data-en="Fairness, Bias and Ethical Principles" data-gr="Δικαιοσύνη, Προκατάληψη και Ηθικές Αρχές">Fairness, Bias and Ethical Principles</h2>
<p data-en="Beyond compliance, responsible AI requires ensuring that systems do not entrench discrimination or harm marginalised groups. Numerous studies have shown that AI models trained on unbalanced data can produce biased outcomes. For example, medical AI trained predominantly on light‑skinned images may perform poorly for darker‑skinned patients, leading to unequal care【962794753570471†L430-L454】. The UK Information Commissioner’s Office warns that algorithms can discriminate based on gender, race, age or other characteristics if bias in training data is not addressed【183687103680724†L95-L112】. The fairness principle therefore obliges developers to implement technical and organisational measures to mitigate bias, to use diverse training datasets, and to process personal data in ways people reasonably expect【183687103680724†L95-L112】【183687103680724†L135-L147】. Transparency and explainability are also vital so that users and regulators can understand how AI systems make decisions. Ethical codes emphasise accountability, human oversight and the right to contest automated decisions. In 2025, fairness and bias mitigation are central themes in AI governance frameworks, complementing the legal obligations of the AI Act." data-gr="Πέρα από τη συμμόρφωση, η υπεύθυνη AI απαιτεί να διασφαλίζεται ότι τα συστήματα δεν παγιώνουν τις διακρίσεις ή δεν βλάπτουν περιθωριοποιημένες ομάδες. Πολλές μελέτες έχουν δείξει ότι τα μοντέλα AI που εκπαιδεύονται με μη ισορροπημένα δεδομένα μπορούν να παράγουν προκατειλημμένα αποτελέσματα. Για παράδειγμα, η ιατρική AI που εκπαιδεύεται κυρίως σε εικόνες ανοιχτόχρωμου δέρματος μπορεί να αποδίδει φτωχά σε ασθενείς με πιο σκούρο δέρμα, οδηγώντας σε άνιση φροντίδα【962794753570471†L430-L454】. Το Γραφείο Επιτρόπου Πληροφοριών του Ηνωμένου Βασιλείου προειδοποιεί ότι οι αλγόριθμοι μπορούν να κάνουν διακρίσεις με βάση το φύλο, τη φυλή, την ηλικία ή άλλα χαρακτηριστικά εάν η προκατάληψη στα δεδομένα εκπαίδευσης δεν αντιμετωπιστεί【183687103680724†L95-L112】. Η αρχή της δικαιοσύνης απαιτεί συνεπώς από τους αναπτύκτες να εφαρμόζουν τεχνικά και οργανωτικά μέτρα για τη μείωση της προκατάληψης, να χρησιμοποιούν ποικίλα σύνολα εκπαίδευσης και να επεξεργάζονται προσωπικά δεδομένα με τρόπο που οι άνθρωποι εύλογα αναμένουν【183687103680724†L95-L112】【183687103680724†L135-L147】. Η διαφάνεια και η εξηγησιμότητα είναι επίσης ζωτικής σημασίας ώστε οι χρήστες και οι ρυθμιστικές αρχές να κατανοούν πώς τα συστήματα AI λαμβάνουν αποφάσεις. Οι ηθικοί κώδικες τονίζουν την λογοδοσία, την ανθρώπινη εποπτεία και το δικαίωμα αμφισβήτησης αυτοματοποιημένων αποφάσεων. Το 2025, η δικαιοσύνη και η μείωση της προκατάληψης αποτελούν κεντρικά θέματα στα πλαίσια διακυβέρνησης AI, συμπληρώνοντας τις νομικές υποχρεώσεις του Νόμου AI.">Beyond compliance, responsible AI requires ensuring that systems do not entrench discrimination or harm marginalised groups. Numerous studies have shown that AI models trained on unbalanced data can produce biased outcomes. For example, medical AI trained predominantly on light‑skinned images may perform poorly for darker‑skinned patients, leading to unequal care【962794753570471†L430-L454】. The UK Information Commissioner’s Office warns that algorithms can discriminate based on gender, race, age or other characteristics if bias in training data is not addressed【183687103680724†L95-L112】. The fairness principle therefore obliges developers to implement technical and organisational measures to mitigate bias, to use diverse training datasets, and to process personal data in ways people reasonably expect【183687103680724†L95-L112】【183687103680724†L135-L147】. Transparency and explainability are also vital so that users and regulators can understand how AI systems make decisions. Ethical codes emphasise accountability, human oversight and the right to contest automated decisions. In 2025, fairness and bias mitigation are central themes in AI governance frameworks, complementing the legal obligations of the AI Act.</p>
</section>
<section class="content-section">
<h2 data-en="Responsible AI Governance and Standards" data-gr="Υπεύθυνη Διακυβέρνηση AI και Πρότυπα">Responsible AI Governance and Standards</h2>
<p data-en="As AI is embedded into critical systems, organisations need structured processes to manage its development and deployment. ISO/IEC 42001, the first international standard for AI management systems, came into force in 2025. It requires organisations to document data sources, perform risk assessments, monitor AI systems throughout their life cycles and ensure continual improvement【945144035004864†L267-L274】. Adopting this standard can help companies align with the AI Act’s requirements and demonstrate due diligence. Alongside ISO 42001, other frameworks encourage responsible innovation, such as AI ethics guidelines, model cards and algorithmic impact assessments. DORA and NIS2 integrate AI into digital resilience and cyber hygiene, recognising that algorithmic failures can have systemic impacts. Organisations should establish interdisciplinary governance bodies to oversee AI projects, including legal, data protection, security and ethics experts. Training and awareness across teams are essential to embed ethical considerations into design and operation. Through proactive governance, businesses can harness AI’s benefits while mitigating risks." data-gr="Καθώς η AI ενσωματώνεται σε κρίσιμα συστήματα, οι οργανισμοί χρειάζονται δομημένες διαδικασίες για τη διαχείριση της ανάπτυξης και της ανάπτυξής της. Το ISO/IEC 42001, το πρώτο διεθνές πρότυπο για συστήματα διαχείρισης AI, τέθηκε σε ισχύ το 2025. Απαιτεί από τους οργανισμούς να τεκμηριώνουν τις πηγές δεδομένων, να πραγματοποιούν αξιολογήσεις κινδύνου, να παρακολουθούν τα συστήματα AI καθ’ όλη τη διάρκεια του κύκλου ζωής τους και να εξασφαλίζουν συνεχή βελτίωση【945144035004864†L267-L274】. Η υιοθέτηση αυτού του προτύπου μπορεί να βοηθήσει τις εταιρείες να ευθυγραμμιστούν με τις απαιτήσεις του Νόμου AI και να επιδείξουν επιμέλεια. Μαζί με το ISO 42001, άλλα πλαίσια ενθαρρύνουν υπεύθυνη καινοτομία, όπως οι ηθικές οδηγίες AI, τα “model cards” και οι εκτιμήσεις αντίκτυπου αλγορίθμων. Το DORA και το NIS2 ενσωματώνουν την AI στην ψηφιακή ανθεκτικότητα και την υγιεινή κυβερνοασφάλειας, αναγνωρίζοντας ότι οι αποτυχίες των αλγορίθμων μπορούν να έχουν συστημικές επιπτώσεις. Οι οργανισμοί πρέπει να ιδρύσουν διεπιστημονικά όργανα διακυβέρνησης για την επίβλεψη έργων AI, συμπεριλαμβανομένων νομικών, υπεύθυνων προστασίας δεδομένων, ειδικών ασφάλειας και ηθικών. Η εκπαίδευση και η ευαισθητοποίηση σε όλες τις ομάδες είναι απαραίτητες για την ενσωμάτωση ηθικών παραμέτρων στον σχεδιασμό και τη λειτουργία. Μέσω προληπτικής διακυβέρνησης, οι επιχειρήσεις μπορούν να αξιοποιήσουν τα οφέλη της AI ενώ μετριάζουν τους κινδύνους.">As AI is embedded into critical systems, organisations need structured processes to manage its development and deployment. ISO/IEC 42001, the first international standard for AI management systems, came into force in 2025. It requires organisations to document data sources, perform risk assessments, monitor AI systems throughout their life cycles and ensure continual improvement【945144035004864†L267-L274】. Adopting this standard can help companies align with the AI Act’s requirements and demonstrate due diligence. Alongside ISO 42001, other frameworks encourage responsible innovation, such as AI ethics guidelines, model cards and algorithmic impact assessments. DORA and NIS2 integrate AI into digital resilience and cyber hygiene, recognising that algorithmic failures can have systemic impacts. Organisations should establish interdisciplinary governance bodies to oversee AI projects, including legal, data protection, security and ethics experts. Training and awareness across teams are essential to embed ethical considerations into design and operation. Through proactive governance, businesses can harness AI’s benefits while mitigating risks.</p>
</section>
<section class="content-section">
<h2 data-en="Global Harmonization and Compliance Challenges" data-gr="Παγκόσμια Εναρμόνιση και Προκλήσεις Συμμόρφωσης">Global Harmonization and Compliance Challenges</h2>
<p data-en="The rapid proliferation of AI legislation creates a patchwork of obligations that organisations must navigate. The EU AI Act prohibits social scoring and manipulative techniques【945144035004864†L149-L157】; Canada’s forthcoming Artificial Intelligence and Data Act and Quebec’s Law 25 impose oversight and consent requirements; India’s DPDPA emphasises notice, consent and fiduciary obligations【945144035004864†L209-L217】; U.S. state laws grant individuals rights to opt out of profiling and targeted advertising【945144035004864†L231-L244】; and potential updates to COPPA aim to protect teens from AI profiling【945144035004864†L284-L290】. Differences in definitions, risk classifications and enforcement create complexity. High‑risk AI under the EU Act may be subject to different standards in other jurisdictions. To stay compliant, multinational companies must map regulatory requirements, localise risk assessments and tailor governance accordingly. At the same time, policymakers are discussing interoperability and mutual recognition. As AI becomes ubiquitous, there is growing recognition that international coordination is necessary to avoid fragmentation and to protect human rights globally. Developing voluntary codes of conduct and aligning on core principles such as fairness, transparency, accountability and human oversight can help bridge regulatory gaps." data-gr="Η ταχεία εξάπλωση της νομοθεσίας για την AI δημιουργεί ένα μωσαϊκό υποχρεώσεων που οι οργανισμοί πρέπει να αντιμετωπίσουν. Ο Νόμος AI της ΕΕ απαγορεύει το κοινωνικό σκόρ και τις χειριστικές τεχνικές【945144035004864†L149-L157】· ο επερχόμενος Νόμος AI και Δεδομένων του Καναδά και ο Νόμος 25 του Κεμπέκ επιβάλλουν εποπτεία και απαιτήσεις συναίνεσης· ο ινδικός DPDPA δίνει έμφαση στην ειδοποίηση, τη συναίνεση και τις υποχρεώσεις εμπιστοσύνης【945144035004864†L209-L217】· οι πολιτειακοί νόμοι των ΗΠΑ παρέχουν στα άτομα δικαίωμα άρνησης συμμετοχής στο προφίλ και τη στοχευμένη διαφήμιση【945144035004864†L231-L244】· και πιθανές αναθεωρήσεις του COPPA στοχεύουν στην προστασία των εφήβων από το προφίλ AI【945144035004864†L284-L290】. Οι διαφορές στους ορισμούς, τις κατηγορίες κινδύνου και την εφαρμογή δημιουργούν πολυπλοκότητα. Η AI υψηλού κινδύνου σύμφωνα με τον Νόμο της ΕΕ μπορεί να υπόκειται σε διαφορετικά πρότυπα σε άλλες δικαιοδοσίες. Για να παραμείνουν συμμορφωμένες, οι πολυεθνικές εταιρείες πρέπει να χαρτογραφήσουν τις κανονιστικές απαιτήσεις, να προσαρμόσουν τις αξιολογήσεις κινδύνου σε τοπικό επίπεδο και να προσαρμόσουν τη διακυβέρνηση ανάλογα. Ταυτόχρονα, οι φορείς χάραξης πολιτικής συζητούν τη διαλειτουργικότητα και την αμοιβαία αναγνώριση. Καθώς η AI γίνεται πανταχού παρούσα, αναγνωρίζεται ολοένα και περισσότερο ότι η διεθνής συνεργασία είναι απαραίτητη για την αποφυγή κατακερματισμού και την προστασία των ανθρωπίνων δικαιωμάτων σε παγκόσμιο επίπεδο. Η ανάπτυξη εθελοντικών κωδίκων δεοντολογίας και η ευθυγράμμιση σε βασικές αρχές όπως η δικαιοσύνη, η διαφάνεια, η λογοδοσία και η ανθρώπινη επίβλεψη μπορούν να βοηθήσουν να γεφυρωθούν τα ρυθμιστικά κενά.">The rapid proliferation of AI legislation creates a patchwork of obligations that organisations must navigate. The EU AI Act prohibits social scoring and manipulative techniques【945144035004864†L149-L157】; Canada’s forthcoming Artificial Intelligence and Data Act and Quebec’s Law 25 impose oversight and consent requirements; India’s DPDPA emphasises notice, consent and fiduciary obligations【945144035004864†L209-L217】; U.S. state laws grant individuals rights to opt out of profiling and targeted advertising【945144035004864†L231-L244】; and potential updates to COPPA aim to protect teens from AI profiling【945144035004864†L284-L290】. Differences in definitions, risk classifications and enforcement create complexity. High‑risk AI under the EU Act may be subject to different standards in other jurisdictions. To stay compliant, multinational companies must map regulatory requirements, localise risk assessments and tailor governance accordingly. At the same time, policymakers are discussing interoperability and mutual recognition. As AI becomes ubiquitous, there is growing recognition that international coordination is necessary to avoid fragmentation and to protect human rights globally. Developing voluntary codes of conduct and aligning on core principles such as fairness, transparency, accountability and human oversight can help bridge regulatory gaps.</p>
</section>
<section class="content-section">
<h2 data-en="Sources" data-gr="Πηγές">Sources</h2>
<ul>
<li data-en="The EU AI Act bans manipulative techniques, social scoring and real‑time biometric surveillance【945144035004864†L149-L157】." data-gr="Ο Νόμος AI της ΕΕ απαγορεύει τις χειριστικές τεχνικές, το κοινωνικό σκόρ και τη βιομετρική επιτήρηση σε πραγματικό χρόνο【945144035004864†L149-L157】.">The EU AI Act bans manipulative techniques, social scoring and real‑time biometric surveillance【945144035004864†L149-L157】.</li>
<li data-en="High‑risk AI systems must implement risk management, data governance, human oversight and monitoring【960195212561326†L1000-L1023】." data-gr="Τα συστήματα AI υψηλού κινδύνου πρέπει να εφαρμόσουν διαχείριση κινδύνου, διακυβέρνηση δεδομένων, ανθρώπινη εποπτεία και παρακολούθηση【960195212561326†L1000-L1023】.">High‑risk AI systems must implement risk management, data governance, human oversight and monitoring【960195212561326†L1000-L1023】.</li>
<li data-en="AI models can be biased when trained on unbalanced data and can discriminate based on gender, race or age【962794753570471†L430-L454】【183687103680724†L95-L112】." data-gr="Τα μοντέλα AI μπορούν να είναι προκατειλημμένα όταν εκπαιδεύονται με μη ισορροπημένα δεδομένα και να κάνουν διακρίσεις με βάση το φύλο, τη φυλή ή την ηλικία【962794753570471†L430-L454】【183687103680724†L95-L112】.">AI models can be biased when trained on unbalanced data and can discriminate based on gender, race or age【962794753570471†L430-L454】【183687103680724†L95-L112】.</li>
<li data-en="The UK Information Commissioner’s Office emphasises fairness, bias mitigation and reasonable expectations in AI processing【183687103680724†L95-L112】【183687103680724†L135-L147】." data-gr="Το Γραφείο Επιτρόπου Πληροφοριών του Ηνωμένου Βασιλείου υπογραμμίζει τη δικαιοσύνη, τη μείωση της προκατάληψης και τις εύλογες προσδοκίες στην επεξεργασία AI【183687103680724†L95-L112】【183687103680724†L135-L147】.">The UK Information Commissioner’s Office emphasises fairness, bias mitigation and reasonable expectations in AI processing【183687103680724†L95-L112】【183687103680724†L135-L147】.</li>
<li data-en="ISO/IEC 42001 requires documentation, risk assessment and lifecycle monitoring for AI systems【945144035004864†L267-L274】." data-gr="Το ISO/IEC 42001 απαιτεί τεκμηρίωση, αξιολόγηση κινδύνου και παρακολούθηση κύκλου ζωής για τα συστήματα AI【945144035004864†L267-L274】.">ISO/IEC 42001 requires documentation, risk assessment and lifecycle monitoring for AI systems【945144035004864†L267-L274】.</li>
</ul>
</section>
</div>
</article>
<footer class="main-footer">
<p data-lang-section="en">© 2026 DedSec Project. All Rights Reserved.</p>
<p class="hidden-by-default" data-lang-section="gr">© 2026 DedSec Project. Με επιφύλαξη παντός δικαιώματος.</p>
<p>Made by dedsec1121fk.</p>
</footer>

<script defer="" src="../script.js"></script>
</body>
</html>
